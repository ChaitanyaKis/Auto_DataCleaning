# ğŸ“¦ Automated Data Cleaning Pipeline

A reusable, dataset-agnostic data cleaning solution that standardizes, imputes, and prepares structured data for analysis or machine learning workflows with zero manual configuration.

---

## ğŸ“Œ Purpose

This repository provides a fully automated data cleaning engine capable of:

- Detecting input datasets automatically
- Removing high-null columns and duplicate rows
- Normalizing text features
- Handling missing values intelligently
- Detecting and capping outliers
- Correcting skewed distributions
- Optimizing memory usage

The output is a clean, structured CSV ready for analytics, modeling, or visualization.

---

## ğŸ§© Features

### âœ” Automatic Dataset Detection
Automatically selects the first CSV file found in:

```
data/raw/
```

---

### âœ” Column Removal Based on Missing Data
Columns containing more than **70% missing values** are automatically dropped.

---

### âœ” Duplicate Handling
Removes exact duplicate rows to ensure dataset integrity.

---

### âœ” Text Normalization
Standardizes string values by:
- trimming whitespace  
- converting to lowercase  
- handling empty values  

---

### âœ” Smart Type Conversion
Attempts numeric conversion when appropriate while preserving valid categorical values.

---

### âœ” Missing Value Strategy
| Data Type | Strategy |
|--------|----------|
Numeric | Median Imputation |
Categorical | `"missing"` placeholder |

---

### âœ” Outlier Capping (IQR Method)
Extreme values are capped instead of removed, preserving dataset size.

---

### âœ” Skewness Correction
Automatically applies log transformation to highly skewed numerical features.

---

### âœ” Memory Optimization
Reduces memory footprint by:
- downcasting numeric types
- converting suitable text columns to categorical dtype

---

## ğŸ“ Project Structure

## ğŸ“ Project Structure

```
Task_1 - DataCleaning/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Place any CSV dataset here
â”‚   â””â”€â”€ output/         # Cleaned dataset saved here
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cleaner.py      # Core cleaning pipeline
â”‚   â”œâ”€â”€ profiler.py     # Dataset profiling logic
â”‚   â”œâ”€â”€ optimizer.py    # Memory optimization utilities
â”‚   â”œâ”€â”€ encoder.py      # Categorical encoding module
â”‚   â”œâ”€â”€ scaler.py       # Feature scaling module
â”‚   â”œâ”€â”€ visualizer.py   # Outlier visualization (boxplots)
â”‚   â””â”€â”€ utils.py        # Helper utilities
â”‚
â”œâ”€â”€ main.py                 # Entry script
â”œâ”€â”€ output_data_quality.py  # Dataset quality evaluation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## âš™ï¸ Setup

Clone the repository and install dependencies:

```bash
python -m venv venv
venv/Scripts/activate
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### 1. Place dataset

Put any `.csv` file inside:

```
data/raw/
```

---

### 2. Run pipeline

```bash
python main.py
```

---

### 3. Output location

The cleaned dataset will be saved automatically to:

```
data/output/cleaned_data.csv
```

---

## ğŸ“Š Dataset Quality Evaluation

After cleaning, evaluate the processed dataset:

```bash
python output_data_quality.py
```

This generates a structured quality report including:

- Missing values
- Duplicate rows
- Outlier count
- Skewed columns
- Scaling validation
- Encoding validation
- Memory usage
- Overall quality score

---

## ğŸ¯ Design Goals

This pipeline is designed to be:

- Dataset-agnostic
- Deterministic
- Reproducible
- Efficient
- Production-ready

No manual preprocessing or dataset-specific configuration is required.

---

## ğŸ“œ License

This project is intended for educational and evaluation purposes.

---

