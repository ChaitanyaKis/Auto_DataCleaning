# ğŸ“¦ Automated Data Cleaning Pipeline

A reusable, dataset-agnostic data cleaning solution that standardizes, imputes, and prepares structured data for analysis or machine learning workflows with zero manual configuration.

---

## ğŸ“Œ Purpose

This repository provides a fully automated data cleaning engine capable of:

- Detecting input dataset
- Removing high-null and duplicate entries
- Normalizing text fields
- Handling missing values intelligently
- Detecting and capping outliers
- Correcting skewness
- Optimizing memory usage

The outcome is a production-ready, cleaned CSV suitable for downstream tasks.

---

## ğŸ§© Features

### âœ” Automatic Dataset Detection
Automatically picks the first CSV file found in `data/raw/`.

### âœ” Column Dropping Based on Missing Data
Columns with >70% missing values are removed.

### âœ” Duplicate Handling
Removes exact duplicates.

### âœ” Text Normalization
Cleans strings (trim, lower-case, missing categorization).

### âœ” Smart Type Conversion
Attempts numeric conversion when appropriate.

### âœ” Missing Value Strategy
- Numerical â†’ Median
- Categorical â†’ â€œmissingâ€

### âœ” Outlier Capping (IQR)
Limits extreme values rather than removing data.

### âœ” Skewness Correction
Applies log transformation for numeric columns with significant skew.

### âœ” Memory Optimization
Downcasts numeric types and converts suitable text to category.

---

## ğŸ“ Folder Structure

```
Task_1 - DataCleaning/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/        # Place any CSV dataset here
â”‚   â””â”€â”€ output/     # Cleaned output saved here
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cleaner.py      # Core pipeline
â”‚   â”œâ”€â”€ profiler.py     # Dataset analysis
â”‚   â”œâ”€â”€ optimizer.py    # Memory optimizer
â”‚   â””â”€â”€ utils.py        # Helpers
â”‚
â”œâ”€â”€ main.py                 # Entry script
â”œâ”€â”€ output_data_quality.py  # Output quality evaluation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```




---

## ğŸš€ Setup

1. Clone repository
2. Create and activate virtual environment
3. Install dependencies


python -m venv venv
venv/Scripts/activate
pip install -r requirements.txt


## â–¶ï¸ Usage

1. Place any `.csv` file inside:

data/raw/

2. Run the cleaning pipeline:

python main.pu

3. The cleaned dataset will be saved to:

data/output/cleaned_data.csv
